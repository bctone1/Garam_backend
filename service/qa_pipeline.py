# # í˜„ì¬ ì‚¬ìš© ì¤‘ì§€ì„ì‹œ ì½”ë“œ
# ## ëŒ€í™”ê¸°ë¡ ì—¬ë¶€ ê²€í† 
# ### sessionì•ˆì—ì„œ ë¦¬íŠ¸ë¦¬ë²„ ì—­í• ì„ í•  ìˆ˜ ë„ ìˆëŠ”ë° ë¡œê·¸ì¸ ë°©ì‹ì´ ì•„ë‹ˆë¼ ì„¸ì…˜ë³„ë¡œ ê¸°ì–µí•  í•„ìš”ê°€ ì—†ë‹¤ëŠ” ì ì´ ìš°ë ¤ë˜ë‚˜
# #### ê´€ë¦¬ìê°€ ì–´ë– í•œ ì§ˆë¬¸ì´ ë“¤ì–´ì™”ëŠ”ì§€ í™•ì¸í•´ì•¼ í•˜ë¯€ë¡œ ëª¨ë‘ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì€ DBí™” í•˜ì—¬ ì €ì¥í•´ ë‘ì–´ì•¼ í•œë‹¤.
# from typing import Tuple
# from sqlalchemy.orm import Session
# from langchain.chains import RetrievalQA
# from langchain_core.prompts import PromptTemplate
# from langchain_core.output_parsers import StrOutputParser
# from langchain_service.chain.qa_chain import make_qa_chain
# from core import config
#
# from langchain_service.embedding.get_vector import text_to_vector
# from crud import chat as crud_chat
# from crud import knowledge as crud_knowledge
# from langchain_service.llm.setup import get_llm
#
#
# class QAPipeline:
#     def __init__(self, provider=None, model=None, api_key=None, top_k=3):
#         # provider = provider or getattr(config, "LLM_PROVIDER", "openai")
#         # model    = model    or getattr(config, "LLM_MODEL", getattr(config, "DEFAULT_CHAT_MODEL", "gpt-4o-mini"))
#         provider = provider or getattr(config, "LLM_PROVIDER")
#         model = model or getattr(config, "LLM_MODEL")
#
#         self.provider = provider
#         self.model = model
#         self.llm = get_llm(provider, model, api_key=api_key)
#         self.top_k = top_k
#
#
#         self.prompt = PromptTemplate(
#             input_variables=["history", "context", "input"],
#             template=(
#                 "ì•„ë˜ ë¬¸ë§¥ê³¼ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\n"
#                 "ë¬¸ë§¥:\n{context}\n\n"
#                 "ëŒ€í™” íˆìŠ¤í† ë¦¬:\n{history}\n\n"
#                 "ì§ˆë¬¸: {input}\n\n"
#                 "AI:"
#             ),
#         )
#
#     def answer(self, db: Session, session_id: int, question: str) -> str:
#         # 1. ì§ˆë¬¸ ë²¡í„°í™”
#         q_vector = text_to_vector(question)
#
#         # 2. ì§€ì‹ë² ì´ìŠ¤ ê²€ìƒ‰
#         docs = crud_knowledge.search_chunks(db, q_vector, top_k=self.top_k)
#         context = "\n".join([d.chunk_text for d in docs])
#
#         # 3. ëŒ€í™” ê¸°ë¡ ê²€ìƒ‰
#         history_msgs = crud_chat.get_relevant_messages(db, session_id, q_vector, top_n=5)
#         history = "\n".join([f"{m['role'].capitalize()}: {m['content']}" for m in history_msgs])
#
#         # 4. ì²´ì¸ ì‹¤í–‰
#         # chain = make_qa_chain(db, get_llm, text_to_vector)
#         chain = self.prompt | self.llm | StrOutputParser()
#
#         # ğŸ” LangSmith í‘œì‹œìš© ì„¤ì •(ëŸ° ì´ë¦„/íƒœê·¸/ë©”íƒ€ë°ì´í„°)
#         configured = chain.with_config({
#             "run_name": "RAG QA",
#             "tags": [
#                 "qapipeline",
#                 f"provider:{self.provider}",
#                 f"model:{self.model}"
#             ],
#             "metadata": {
#                 "session_id": session_id,
#                 "top_k": self.top_k,
#                 "doc_ids": [getattr(d, "id", None) for d in docs]
#             }
#         })
#
#
#         answer = configured.invoke({"history": history, "context": context, "input": question})
#
#         # 5. ëŒ€í™” ë¡œê·¸ ì €ì¥
#         crud_chat.save_message(db, session_id, "user", question, q_vector)
#         crud_chat.save_message(db, session_id, "assistant", answer, None)
#
#         return answer
